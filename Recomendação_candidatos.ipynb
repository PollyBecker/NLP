{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl6Lm3DPOGPGmys5y3PnbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PollyBecker/NLP/blob/main/Recomenda%C3%A7%C3%A3o_candidatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recomendando candidatos"
      ],
      "metadata": {
        "id": "ughiFylnlV5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "data = {\n",
        "    'vaga_descricao': [\n",
        "        \"Estamos buscando um Desenvolvedor Python para integrar nossa equipe de tecnologia. \"\n",
        "        \"Experiência em Django, APIs REST e microserviços é essencial. Boa comunicação e \"\n",
        "        \"capacidade de resolver problemas são diferenciais.\",\n",
        "\n",
        "        \"Procuramos um Engenheiro de Dados com prática em Hadoop, Spark e bancos SQL. \"\n",
        "        \"Otimizar pipelines de dados e colaborar com equipes será parte do trabalho.\",\n",
        "\n",
        "        \"Vaga para Analista de Marketing Digital com foco em SEO e campanhas pagas. \"\n",
        "        \"Conhecimento em Google Analytics e gestão de campanhas é essencial.\",\n",
        "\n",
        "        \"Buscamos Designer Gráfico com experiência em branding e design de interfaces. \"\n",
        "        \"Domínio de ferramentas Adobe e trabalho em equipe são necessários.\",\n",
        "\n",
        "        \"Engenheiro de Software para desenvolvimento full-stack. Necessário conhecimento \"\n",
        "        \"em Node.js, React e bancos de dados relacionais e não-relacionais.\",\n",
        "\n",
        "        \"Vaga para Cientista de Dados com experiência em machine learning e análise preditiva. \"\n",
        "        \"Conhecimento em Python e R será um diferencial.\",\n",
        "\n",
        "        \"Estamos contratando um Gerente de Projetos com certificação PMP e experiência em \"\n",
        "        \"gestão de equipes ágeis.\",\n",
        "\n",
        "        \"Procuramos um Administrador de Sistemas com conhecimento em Linux, Docker e Kubernetes. \"\n",
        "        \"Experiência com ambientes de alta disponibilidade é essencial.\",\n",
        "\n",
        "        \"Buscamos Desenvolvedor Mobile com foco em Flutter e React Native. Experiência com \"\n",
        "        \"publicação de apps nas lojas oficiais é desejável.\",\n",
        "\n",
        "        \"Vaga para Especialista em Segurança da Informação. Conhecimento em criptografia, \"\n",
        "        \"firewalls e normas ISO 27001 é necessário.\",\n",
        "\n",
        "        \"Analista de RH com experiência em recrutamento e gestão de desempenho. Será um \"\n",
        "        \"diferencial conhecimento em plataformas de gestão de talentos.\",\n",
        "\n",
        "        \"Estamos buscando um Arquiteto de Soluções com foco em cloud computing e arquitetura \"\n",
        "        \"orientada a microserviços.\",\n",
        "\n",
        "        \"Vaga para Consultor de Vendas B2B com experiência em negociações complexas e CRM.\",\n",
        "\n",
        "        \"Procuramos um Analista de Suporte com conhecimento em redes e atendimento ao cliente.\",\n",
        "\n",
        "        \"Estamos contratando um Especialista em UX/UI para trabalhar em produtos digitais \"\n",
        "        \"inovadores.\",\n",
        "\n",
        "        \"Engenheiro Civil com experiência em obras de grande porte e gestão de projetos de \"\n",
        "        \"infraestrutura.\",\n",
        "\n",
        "        \"Buscamos um Advogado Corporativo com foco em direito societário e contratos empresariais.\",\n",
        "\n",
        "        \"Procuramos um Analista Financeiro para atuar com planejamento e controle orçamentário.\",\n",
        "\n",
        "        \"Estamos buscando um Técnico de TI com conhecimento em manutenção de hardware e suporte.\",\n",
        "\n",
        "        \"Vaga para Gerente de Marketing com foco em estratégias digitais e branding.\"\n",
        "    ],\n",
        "\n",
        "    'candidato_descricao': [\n",
        "        \"Experiência com desenvolvimento Python e Django. Trabalhou em projetos web e APIs. \"\n",
        "        \"Forte em integração de sistemas e automação de processos.\",\n",
        "\n",
        "        \"Engenheiro de Dados com domínio de Hadoop e Spark. Experiência em otimização de pipelines.\",\n",
        "\n",
        "        \"Profissional de marketing com experiência em SEO e campanhas pagas no Google e Facebook Ads.\",\n",
        "\n",
        "        \"Designer com prática em criação de identidade visual e interfaces para aplicativos móveis.\",\n",
        "\n",
        "        \"Desenvolvedor full-stack com domínio em Node.js e React. Atuou em projetos de e-commerce.\",\n",
        "\n",
        "        \"Cientista de dados com foco em análise preditiva e machine learning usando Python.\",\n",
        "\n",
        "        \"Gerente de projetos certificado PMP. Experiência na condução de times ágeis.\",\n",
        "\n",
        "        \"Administrador de sistemas com conhecimento avançado em Linux, Docker e Kubernetes.\",\n",
        "\n",
        "        \"Desenvolvedor mobile especializado em Flutter. Publicou apps para Android e iOS.\",\n",
        "\n",
        "        \"Especialista em segurança da informação com experiência em normas ISO e criptografia.\",\n",
        "\n",
        "        \"Analista de RH com atuação em recrutamento e gestão de talentos por meio de sistemas ERP.\",\n",
        "\n",
        "        \"Arquiteto de soluções com foco em cloud e infraestrutura orientada a microserviços.\",\n",
        "\n",
        "        \"Consultor de vendas B2B com experiência em negociação de contratos de longo prazo.\",\n",
        "\n",
        "        \"Analista de suporte técnico com conhecimento em redes e experiência em helpdesk.\",\n",
        "\n",
        "        \"Especialista em UX/UI com foco em usabilidade e criação de protótipos interativos.\",\n",
        "\n",
        "        \"Engenheiro civil com experiência em gestão de obras e controle de cronograma.\",\n",
        "\n",
        "        \"Advogado corporativo com atuação em contratos e direito empresarial.\",\n",
        "\n",
        "        \"Analista financeiro com foco em planejamento orçamentário e análise de custos.\",\n",
        "\n",
        "        \"Técnico de TI com experiência em suporte a hardware e software.\",\n",
        "\n",
        "        \"Gerente de marketing com atuação em campanhas digitais e gestão de marca.\"\n",
        "    ],\n",
        "\n",
        "    'candidato_nivel': [\n",
        "        3, 4, 3, 3, 4, 4, 4, 3, 3, 4,\n",
        "        2, 3, 4, 2, 4, 4, 4, 3, 2, 4\n",
        "    ],\n",
        "\n",
        "    'candidato_formacao': [\n",
        "        \"Engenharia de Software\", \"Ciência de Dados\", \"Marketing\", \"Design Gráfico\",\n",
        "        \"Engenharia da Computação\", \"Estatística\", \"Administração\", \"Sistemas de Informação\",\n",
        "        \"Engenharia de Software\", \"Segurança da Informação\", \"Psicologia\", \"Engenharia de Software\",\n",
        "        \"Administração de Empresas\", \"Redes de Computadores\", \"Design\", \"Engenharia Civil\",\n",
        "        \"Direito\", \"Economia\", \"Técnico em Informática\", \"Publicidade e Propaganda\"\n",
        "    ],\n",
        "\n",
        "    'contratado': [1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "kvaSuGTtlagW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unicode\n",
        "!pip install unidecode\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-EXiu4rqHR_",
        "outputId": "1ae9d834-fceb-43a0-d9d3-dd82eb2064f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unicode\n",
            "  Downloading unicode-2.9-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading unicode-2.9-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: unicode\n",
            "Successfully installed unicode-2.9\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from unidecode import unidecode\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Baixar os recursos necessários do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Baixar o modelo Spacy para português (execute apenas uma vez se necessário)\n",
        "#!python -m spacy download pt_core_news_sm\n",
        "\n",
        "# Carregar o modelo do Spacy em português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Função de pré-processamento de texto com lematização\n",
        "def preprocess_text(text):\n",
        "    text = unidecode(text)  # Remover acentos\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remover caracteres especiais\n",
        "    tokens = word_tokenize(text.lower())  # Tokenizar e converter para minúsculas\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Manter apenas palavras alfabéticas\n",
        "    stop_words = set(stopwords.words('portuguese'))  # Stopwords em português\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remover stopwords\n",
        "\n",
        "    # Lematizar os tokens usando Spacy\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "# Criar DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Aplicar o pré-processamento nos textos\n",
        "df['vaga_descricao_limpa'] = df['vaga_descricao'].apply(preprocess_text)\n",
        "df['candidato_descricao_limpa'] = df['candidato_descricao'].apply(preprocess_text)\n",
        "\n",
        "# Vetorização TF-IDF com no máximo 5000 features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix_vagas = tfidf_vectorizer.fit_transform(df['vaga_descricao_limpa'])\n",
        "tfidf_matrix_candidatos = tfidf_vectorizer.transform(df['candidato_descricao_limpa'])\n",
        "\n",
        "# Função para encontrar os melhores candidatos\n",
        "def encontrar_melhores_candidatos(vaga_idx, df, tfidf_matrix_vagas, tfidf_matrix_candidatos):\n",
        "    # Calcular similaridade cosseno entre a vaga e todos os candidatos\n",
        "    vaga_vector = tfidf_matrix_vagas[vaga_idx]\n",
        "    sim_matrix = cosine_similarity(vaga_vector, tfidf_matrix_candidatos)\n",
        "\n",
        "    # Adicionar as similaridades ao DataFrame e ordenar por relevância\n",
        "    df['similaridade'] = sim_matrix.flatten()\n",
        "    melhores_candidatos = df.sort_values(by='similaridade', ascending=False)\n",
        "\n",
        "    return melhores_candidatos\n",
        "\n",
        "# Testar com a primeira vaga (índice 0)\n",
        "melhores_candidatos = encontrar_melhores_candidatos(16, df, tfidf_matrix_vagas, tfidf_matrix_candidatos)\n",
        "\n",
        "# Exibir os resultados\n",
        "print(melhores_candidatos[['candidato_descricao', 'candidato_nivel', 'similaridade']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiL3Dfxipsbt",
        "outputId": "4cbfaba3-84a5-47ee-8093-be0fbf4f9927"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "                                  candidato_descricao  candidato_nivel  \\\n",
            "0   Experiência com desenvolvimento Python e Djang...                3   \n",
            "8   Desenvolvedor mobile especializado em Flutter....                3   \n",
            "11  Arquiteto de soluções com foco em cloud e infr...                3   \n",
            "4   Desenvolvedor full-stack com domínio em Node.j...                4   \n",
            "12  Consultor de vendas B2B com experiência em neg...                4   \n",
            "6   Gerente de projetos certificado PMP. Experiênc...                4   \n",
            "13  Analista de suporte técnico com conhecimento e...                2   \n",
            "2   Profissional de marketing com experiência em S...                3   \n",
            "15  Engenheiro civil com experiência em gestão de ...                4   \n",
            "9   Especialista em segurança da informação com ex...                4   \n",
            "1   Engenheiro de Dados com domínio de Hadoop e Sp...                4   \n",
            "18  Técnico de TI com experiência em suporte a har...                2   \n",
            "17  Analista financeiro com foco em planejamento o...                3   \n",
            "16  Advogado corporativo com atuação em contratos ...                4   \n",
            "10  Analista de RH com atuação em recrutamento e g...                2   \n",
            "14  Especialista em UX/UI com foco em usabilidade ...                4   \n",
            "7   Administrador de sistemas com conhecimento ava...                3   \n",
            "5   Cientista de dados com foco em análise prediti...                4   \n",
            "3   Designer com prática em criação de identidade ...                3   \n",
            "19  Gerente de marketing com atuação em campanhas ...                4   \n",
            "\n",
            "    similaridade  \n",
            "0       0.223782  \n",
            "8       0.099612  \n",
            "11      0.077081  \n",
            "4       0.071522  \n",
            "12      0.053312  \n",
            "6       0.049483  \n",
            "13      0.049116  \n",
            "2       0.048113  \n",
            "15      0.046036  \n",
            "9       0.043205  \n",
            "1       0.041307  \n",
            "18      0.000000  \n",
            "17      0.000000  \n",
            "16      0.000000  \n",
            "10      0.000000  \n",
            "14      0.000000  \n",
            "7       0.000000  \n",
            "5       0.000000  \n",
            "3       0.000000  \n",
            "19      0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Testar com a primeira vaga (índice 16)\n",
        "melhores_candidatos = encontrar_melhores_candidatos(1, df, tfidf_matrix_vagas, tfidf_matrix_candidatos)\n",
        "\n",
        "# Exibir os resultados\n",
        "print(melhores_candidatos[['candidato_descricao', 'candidato_nivel', 'similaridade']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p50V2SZsYo1",
        "outputId": "8546bdb2-6ecf-4b48-d101-ba00b6e9eda9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  candidato_descricao  candidato_nivel  \\\n",
            "1   Engenheiro de Dados com domínio de Hadoop e Sp...                4   \n",
            "15  Engenheiro civil com experiência em gestão de ...                4   \n",
            "5   Cientista de dados com foco em análise prediti...                4   \n",
            "0   Experiência com desenvolvimento Python e Djang...                3   \n",
            "11  Arquiteto de soluções com foco em cloud e infr...                3   \n",
            "18  Técnico de TI com experiência em suporte a har...                2   \n",
            "17  Analista financeiro com foco em planejamento o...                3   \n",
            "16  Advogado corporativo com atuação em contratos ...                4   \n",
            "14  Especialista em UX/UI com foco em usabilidade ...                4   \n",
            "13  Analista de suporte técnico com conhecimento e...                2   \n",
            "12  Consultor de vendas B2B com experiência em neg...                4   \n",
            "10  Analista de RH com atuação em recrutamento e g...                2   \n",
            "9   Especialista em segurança da informação com ex...                4   \n",
            "8   Desenvolvedor mobile especializado em Flutter....                3   \n",
            "7   Administrador de sistemas com conhecimento ava...                3   \n",
            "6   Gerente de projetos certificado PMP. Experiênc...                4   \n",
            "4   Desenvolvedor full-stack com domínio em Node.j...                4   \n",
            "3   Designer com prática em criação de identidade ...                3   \n",
            "2   Profissional de marketing com experiência em S...                3   \n",
            "19  Gerente de marketing com atuação em campanhas ...                4   \n",
            "\n",
            "    similaridade  \n",
            "1       0.481194  \n",
            "15      0.079232  \n",
            "5       0.063168  \n",
            "0       0.000000  \n",
            "11      0.000000  \n",
            "18      0.000000  \n",
            "17      0.000000  \n",
            "16      0.000000  \n",
            "14      0.000000  \n",
            "13      0.000000  \n",
            "12      0.000000  \n",
            "10      0.000000  \n",
            "9       0.000000  \n",
            "8       0.000000  \n",
            "7       0.000000  \n",
            "6       0.000000  \n",
            "4       0.000000  \n",
            "3       0.000000  \n",
            "2       0.000000  \n",
            "19      0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "qFEKUzJjnYCS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from unidecode import unidecode\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Carregar stopwords uma vez fora das funções\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Função de pré-processamento com inicialização interna do Spacy\n",
        "def preprocess_text(text):\n",
        "    # Inicializar Spacy dentro da função para evitar problemas de serialização\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "    text = unidecode(text)  # Remover acentos\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remover caracteres especiais\n",
        "    tokens = word_tokenize(text.lower())  # Tokenizar e converter para minúsculas\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Manter apenas palavras alfabéticas\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remover stopwords\n",
        "\n",
        "    # Lematizar\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "# Função para cálculo de similaridade\n",
        "def calcular_similaridade(vaga, candidatos):\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([vaga] + candidatos)\n",
        "    vaga_vector = tfidf_matrix[0]\n",
        "    candidatos_vectors = tfidf_matrix[1:]\n",
        "    sim_matrix = cosine_similarity(vaga_vector, candidatos_vectors)\n",
        "    return sim_matrix.flatten()\n",
        "\n",
        "# Exemplo de pré-filtragem (simulação)\n",
        "def pre_filtrar_candidatos(vaga):\n",
        "    # Simulação de pré-filtragem\n",
        "    data = {\n",
        "        'candidato_descricao': [\n",
        "            \"Experiência com Python e Django.\",\n",
        "            \"Engenheiro de Dados com Spark.\",\n",
        "            \"Marketing digital com campanhas em redes sociais.\"\n",
        "        ],\n",
        "        'candidato_nivel': [3, 4, 3],\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Função principal para recomendação\n",
        "def recomendar_candidatos(vaga_descricao):\n",
        "    # 1. Pré-filtrar candidatos\n",
        "    candidatos_df = pre_filtrar_candidatos(vaga_descricao)\n",
        "\n",
        "    # 2. Pré-processar os textos de forma paralela (com threading)\n",
        "    candidatos_df['descricao_limpa'] = Parallel(n_jobs=-1, backend='threading')(\n",
        "        delayed(preprocess_text)(desc) for desc in candidatos_df['candidato_descricao']\n",
        "    )\n",
        "    vaga_limpa = preprocess_text(vaga_descricao)\n",
        "\n",
        "    # 3. Calcular similaridade\n",
        "    similaridades = calcular_similaridade(vaga_limpa, candidatos_df['descricao_limpa'].tolist())\n",
        "\n",
        "    # 4. Adicionar a pontuação de similaridade e ordenar\n",
        "    candidatos_df['similaridade'] = similaridades\n",
        "    melhores_candidatos = candidatos_df.sort_values(by='similaridade', ascending=False)\n",
        "\n",
        "    return melhores_candidatos\n",
        "\n",
        "# Exemplo de uso\n",
        "vaga = \"Buscamos desenvolvedor Python com experiência em Django.\"\n",
        "melhores_candidatos = recomendar_candidatos(vaga)\n",
        "\n",
        "print(melhores_candidatos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17bwEeyHyeDZ",
        "outputId": "cf110a7a-8592-4e67-9214-8ccbf323ed97"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 candidato_descricao  candidato_nivel  \\\n",
            "0                   Experiência com Python e Django.                3   \n",
            "1                     Engenheiro de Dados com Spark.                4   \n",
            "2  Marketing digital com campanhas em redes sociais.                3   \n",
            "\n",
            "                          descricao_limpa  similaridade  \n",
            "0               experiencia python django      0.694626  \n",
            "1                    engenheiro dar spark      0.000000  \n",
            "2  marketing digital campanha rede social      0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b81tMagxiOcF",
        "outputId": "60b15b7d-ffa5-4f9c-c6b1-f09ed50691ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Supondo que você já tenha o DataFrame 'df' com seus dados\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convertendo todos os nomes de colunas para strings\n",
        "df.columns = df.columns.astype(str)\n",
        "\n",
        "# Pré-processamento dos dados\n",
        "df['vaga_descricao'] = df['vaga_descricao'].astype(str)\n",
        "df['candidato_descricao'] = df['candidato_descricao'].astype(str)\n",
        "\n",
        "# Vetorização TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['candidato_descricao'])\n",
        "\n",
        "# Adicionando outros recursos e convertendo para formato esparso compatível\n",
        "X_additional = df[['candidato_nivel', 'candidato_formacao']]\n",
        "X_additional = pd.get_dummies(X_additional, drop_first=True)\n",
        "\n",
        "# Garantindo que todos os dados são numéricos\n",
        "X_additional = X_additional.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Verificando e convertendo tipos de dados compatíveis\n",
        "X_additional_sparse = csr_matrix(X_additional.astype(float).values)\n",
        "\n",
        "# Concatenando os vetores TF-IDF com os outros recursos\n",
        "X = hstack((X_tfidf, X_additional_sparse))\n",
        "\n",
        "# Variável alvo\n",
        "y = df['contratado']\n",
        "\n",
        "# Dividindo os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinamento do modelo\n",
        "model = RandomForestClassifier(n_estimators=300, random_state=42,max_depth= 10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Previsão e Avaliação\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Acurácia: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definição do modelo e dos hiperparâmetros\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'max_features': ['sqrt', 'log2']  # Ajustando os valores válidos para max_features\n",
        "}\n",
        "\n",
        "# Validação cruzada com GridSearch\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Previsão e Avaliação\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Acurácia: {accuracy}\")\n",
        "print(f\"Melhores Hiperparâmetros: {grid_search.best_params_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndcYZEEMl1vS",
        "outputId": "5ebdaedf-14df-4cd0-9922-4086a61cf6c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.25\n",
            "Melhores Hiperparâmetros: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-pNxknaou7b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}